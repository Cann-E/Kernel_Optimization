# ğŸ§  Kernel Optimization

This project implements several CUDA kernels to optimize matrix transpose operations on the GPU. It explores memory coalescing, shared memory usage, loop unrolling, and instruction-level parallelism.

## ğŸš€ Machine Learning Relevance

Matrix transpose is a critical operation in many deep learning frameworks, especially during tensor reshaping and backpropagation. Optimizing this operation can significantly accelerate training and inference times on GPUs, making this project directly applicable to machine learning workloads.

## ğŸ’» Features

- Naive transpose kernel (baseline)
- Coalesced memory access kernel
- Shared memory implementation
- Loop unrolling and instruction-level parallelism (ILP)
- Performance benchmarking with execution time and speedup

## ğŸ§ª How to Run

1. Compile with NVCC:

   ```bash
   nvcc transpose_device.cu -o transpose
   ```

2. Run:

   ```bash
   ./transpose
   ```

## ğŸ“Š Benchmark Results

Includes performance comparisons between naive and optimized kernels for various matrix sizes (e.g. 512x512, 1024x1024).

## ğŸ›  Technologies Used

- CUDA C++
- GPU architecture optimization
- Linux CLI tools

## ğŸ“ Folder Structure

- `transpose_device.cu` â€“ all kernel implementations
- `README.md` â€“ this file

