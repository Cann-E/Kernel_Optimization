# 🚀 Kernel_Optimization

A CUDA-based project focused on optimizing matrix transpose operations. The goal is to compare naive, shared memory, and high-performance GPU kernel implementations for matrix transposition and measure their speedup over CPU and baseline GPU methods.

---

## 🧠 Description

This project implements multiple matrix transpose kernels in CUDA. Starting from a naive implementation, it introduces shared memory usage and culminates in a fully optimized version. Performance is measured across multiple matrix sizes (512x512 to 4096x4096). The optimizations focus on reducing global memory latency, maximizing coalesced access, and improving instruction throughput using GPU-specific techniques.

---

## 🔧 Tech Stack

- CUDA
- C++
- NVCC
- Makefile build system

---

## ✨ Key Features

- ✅ Naive GPU transpose kernel for baseline comparison  
- ✅ Shared memory-based transpose kernel  
- ✅ Fully optimized transpose kernel using:
  - Instruction-level parallelism (ILP)  
  - Loop unrolling  
  - Vectorized memory access  
  - Coalesced reads/writes  
- ✅ CPU-based transpose for reference  
- ✅ Performance output in milliseconds across matrix sizes

---

## 📂 Folder Structure

```
.
├── Makefile                  # Builds the project
├── transpose                 # Final executable
├── transpose_host.cpp        # CPU baseline transpose
├── transpose_device.cu       # CUDA kernels (naive, shmem, optimized)
├── transpose_device.cuh      # Kernel headers
├── ta_utilities.cpp/.hpp     # Utility code
├── README                    # Assignment spec (copied from instructions)
```

---

## 🛠️ Setup & Usage

### ✅ Requirements

- CUDA Toolkit 12.x or later
- NVIDIA GPU with Compute Capability 3.5+
- Linux/macOS environment with `make`

### ⚙️ Build

```bash
make
```

### ▶️ Run

```bash
./transpose
```

### 💡 Example Output

```
Size 512 naive CPU: 0.850624 ms
Size 512 GPU memcpy: 0.043104 ms
Size 512 naive GPU: 0.143360 ms
Size 512 shmem GPU: 0.027648 ms
Size 512 optimal GPU: 0.023392 ms
```

---

## 📸 Output

Command-line output with timings for each kernel version across various matrix sizes. Results include GPU memcpy overhead, CPU vs GPU baseline comparisons, and optimized execution time.

---

## 👥 Contributors

- Can Ercan (@cann-e)

---

