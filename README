Assignment 1

Optimize the matrix transpose kernel to achieve better performance.

Please do not modify the naiveTransposeKernel kernel as it's a baseline reference.
Your main work is in the file transpose_device.cu. 

Tasks to do:
1. complete the shmemTransposeKernel() kernel; read the function comments there;
2. complete the optimalTransposeKernel() kernel; here you can apply any techniques you
   want; the goal is the highest performance. 

To build the binary:
$ make

To run the executable:
$ ./transpose

You'll see some output like this
$ ./transpose
Index of the GPU with the lowest temperature: 0 (60 C)
Time limit for this program set to 10 seconds
Size 512 naive CPU: 0.850624 ms
Size 512 GPU memcpy: 0.043104 ms
Size 512 naive GPU: 0.143360 ms
Size 512 shmem GPU: 0.027648 ms
Size 512 optimal GPU: 0.023392 ms

Size 1024 naive CPU: 6.832416 ms
Size 1024 GPU memcpy: 0.023424 ms
Size 1024 naive GPU: 0.051200 ms
Size 1024 shmem GPU: 0.046080 ms
Size 1024 optimal GPU: 0.044032 ms

Size 2048 naive CPU: 33.075394 ms
Size 2048 GPU memcpy: 0.072256 ms
Size 2048 naive GPU: 0.164864 ms
Size 2048 shmem GPU: 0.167840 ms
Size 2048 optimal GPU: 0.167808 ms

Size 4096 naive CPU: 199.228729 ms
Size 4096 GPU memcpy: 0.222240 ms
Size 4096 naive GPU: 0.618496 ms
Size 4096 shmem GPU: 0.615424 ms
Size 4096 optimal GPU: 0.614400 ms

After you are satisfied with your work, to submit the assignment:

$ make submit

You should not see any errors and you should see a zip file locally and also
see your zip file copied to /mnt/data1/submissions/assignment1

Quiz questions; 


PART 1 //work in progress

Question 1.1: Latency Hiding (5 points)
---------------------------------------

Approximately how many arithmetic instructions does it take to hide the latency
of a single arithmetic instruction on a GK110?

Assume all of the arithmetic instructions are independent (ie have no
instruction dependencies).

You do not need to consider the number of execution cores on the chip.

Hint: What is the latency of an arithmetic instruction? How many instructions
can a GK110 begin issuing in 1 clock cycle (assuming no dependencies)?


Question 1.2: Thread Divergence (6 points)
------------------------------------------

Let the block shape be (32, 32, 1).

(a)
int idx = threadIdx.y + blockSize.y * threadIdx.x;
if (idx % 32 < 16)
    foo();
else
    bar();

Does this code diverge? Why or why not?
Answer:
a) Yes, this code can cause thread divergence because different threads will execute different paths .

(b)
const float pi = 3.14;
float result = 1.0;
for (int i = 0; i < threadIdx.x; i++)
    result *= pi;

Does this code diverge? Why or why not? (This is a bit of a trick question,
either "yes" or "no can be a correct answer with appropriate explanation.)


Question 1.3: Coalesced Memory Access (9 points)
------------------------------------------------

Let the block shape be (32, 32, 1). Let data be a (float *) pointing to global
memory and let data be 128 byte aligned (so data % 128 == 0).

Consider each of the following access patterns.

(a)
data[threadIdx.x + blockSize.x * threadIdx.y] = 1.0;

Is this write coalesced? How many 128 byte cache lines does this write to?

(b)
data[threadIdx.y + blockSize.y * threadIdx.x] = 1.0;

Is this write coalesced? How many 128 byte cache lines does this write to?

(c)
data[1 + threadIdx.x + blockSize.x * threadIdx.y] = 1.0;

Is this write coalesced? How many 128 byte cache lines does this write to?


Question 1.4: Bank Conflicts and Instruction Dependencies (15 points)
---------------------------------------------------------------------

Let's consider multiplying a 32 x 128 matrix with a 128 x 32 element matrix.
This outputs a 32 x 32 matrix. We'll use 32 ** 2 = 1024 threads and each thread
will compute 1 output element. Although its not optimal, for the sake of
simplicity let's use a single block, so grid shape = (1, 1, 1),
block shape = (32, 32, 1).

For the sake of this problem, let's assume both the left and right matrices have
already been stored in shared memory are in column major format. This means the
element in the ith row and jth column is accessible at lhs[i + 32 * j] for the
left hand side and rhs[i + 128 * j] for the right hand side.

This kernel will write to a variable called output stored in shared memory.

Consider the following kernel code:

int i = threadIdx.x;
int j = threadIdx.y;
for (int k = 0; k < 128; k += 2) {
    output[i + 32 * j] += lhs[i + 32 * k] * rhs[k + 128 * j];
    output[i + 32 * j] += lhs[i + 32 * (k + 1)] * rhs[(k + 1) + 128 * j];
}

(a)
Are there bank conflicts in this code? If so, how many ways is the bank conflict
(2-way, 4-way, etc)?

(b)
Expand the inner part of the loop (below)

output[i + 32 * j] += lhs[i + 32 * k] * rhs[k + 128 * j];
output[i + 32 * j] += lhs[i + 32 * (k + 1)] * rhs[(k + 1) + 128 * j];

into "psuedo-assembly" as was done in the coordinate addition example in lecture
4.

There's no need to expand the indexing math, only to expand the loads, stores,
and math. Notably, the operation a += b * c can be computed by a single
instruction called a fused multiply add (FMA), so this can be a single
instruction in your "psuedo-assembly".

Hint: Each line should expand to 5 instructions.
Answer:
b)

(c)
Identify pairs of dependent instructions in your answer to part b.
Answer:
c)

(d)
Rewrite the code given at the beginning of this problem to minimize instruction
dependencies. You can add or delete instructions (deleting an instruction is a
valid way to get rid of a dependency!) but each iteration of the loop must still
process 2 values of k.

Answer:
d)

(e)
Can you think of any other anything else you can do that might make this code
run faster?

Answer:
e)Use loop unrolling and memory coalescing.




PART 2 - Matrix transpose optimization (65 points)
--------------------------------------------------

Optimize the CUDA matrix transpose implementations in transpose_cuda.cu. Read
ALL of the TODO comments. Matrix transpose is a common exercise in GPU
optimization, so do not search for existing GPU matrix transpose code on the
internet.

Your transpose code only need to be able to transpose square matrices where the
side length is a multiple of 64.

The initial implementation has each block of 1024 threads handle a 64x64 block
of the matrix, but you can change anything about the kernel if it helps obtain
better performance.

The main method of transpose.cc already checks for correctness for all transpose
results, so there should be an assertion failure if your kernel produces incorrect
output.

The purpose of the shmemTransposeKernel is to demonstrate proper usage of global
and shared memory. The optimalTransposeKernel should be built on top of
shmemTransposeKernel and should incorporate any "tricks" such as ILP, loop
unrolling, vectorized IO, etc that have been discussed in class.

You can compile and run the code by running

make transpose
./transpose




BONUS (+5 points, maximum set score is 100 even with bonus)
--------------------------------------------------------------------------------

Mathematical scripting environments such as Matlab or Python + Numpy often
encourage expressing algorithms in terms of vector operations because they offer
a convenient and performant interface. For instance, one can add 2 n-component
vectors (a and b) in Numpy with c = a + b.

This is often implemented with something like the following code:

void vec_add(float *left, float *right, float *out, int size) {
    for (int i = 0; i < size; i++)
        out[i] = left[i] + right[i];
}

Consider the code

a = x + y + z

where x, y, z are n-component vectors.

One way this could be computed would be

vec_add(x, y, a, n);
vec_add(a, z, a, n);

In what ways is this code (2 calls to vec_add) worse than the following?

for (int i = 0; i < n; i++)
    a[i] = x[i] + y[i] + z[i];

List at least 2 ways (you don't need more than a sentence or two for each way).
